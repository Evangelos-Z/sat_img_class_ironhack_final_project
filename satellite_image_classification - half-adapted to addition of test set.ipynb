{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lightseagreen\">0. Importing Libraries<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "###import datetime\n",
    "\n",
    "# path and data management\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from random import sample\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# image manipulation/processing\n",
    "import rioxarray\n",
    "from PIL import Image\n",
    "from IPython.display import Image as ipimage\n",
    "import odc\n",
    "\n",
    "# FROM ISI'S REPO\n",
    "import shutil # High-level file operations\n",
    "import random # to generate random samples\n",
    "\n",
    "# Computer Vision\n",
    "import tensorflow as tf # machine learning and neural networks\n",
    "from tensorflow import keras # deep learning and neural networks\n",
    "from tensorflow.keras import layers # layers for neural networks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # real-time data augmentation\n",
    "# NOT FROM ISI'S REPO\n",
    "\n",
    "# classification\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting session parameters\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lightseagreen\">1. Importing Datasets<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color:dimgray\">Data description:</h3>\n",
    "<font color = = \"dimgray\">The full dataset, consisting of 4 folders representing the 4 categories \"cloudy\", \"desert\", \"green area\" and \"water\", with 1500, 1131, 1500 and 1500 images respectively was retrieved from <a href=\"https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification\">Satellite Image Classification</a>.<font>\n",
    "\n",
    "<h3 style = \"color:dimgray\">Goal:</h3>\n",
    "<font color = = \"dimgray\">The use of maching learning algorithm(s) for the classification of the images in the abovementioned categories.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data paths. Note that all the files were initially in the same folders per category and since most of them will \n",
    "# be used for training the model, they were saved the \"train\" folder. Later on they will be sampled to create the validation and test sets\n",
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "cloudy_train = os.path.join(data_path, \"train/cloudy\")\n",
    "desert_train = os.path.join(data_path, \"train/desert\")\n",
    "forest_train = os.path.join(data_path, \"train/green_area\")\n",
    "water_train = os.path.join(data_path, \"train/water\")\n",
    "\n",
    "# defining validation directories to be created\n",
    "cloudy_valid = os.path.join(data_path, \"valid/cloudy\")\n",
    "desert_valid = os.path.join(data_path, \"valid/desert\")\n",
    "forest_valid = os.path.join(data_path, \"valid/green_area\")\n",
    "water_valid = os.path.join(data_path, \"valid/water\")\n",
    "\n",
    "# defining test directory to be created\n",
    "imbalanced_test_dir = os.path.join(data_path, \"test\")\n",
    "\n",
    "# listing image names\n",
    "cloudy_img_lst = os.listdir(\"data/train/cloudy\")\n",
    "desert_img_lst = os.listdir(\"data/train/desert\")\n",
    "forest_img_lst = os.listdir(\"data/train/green_area\")\n",
    "water_img_lst = os.listdir(\"data/train/water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM ISI - creating new directories and ### Making sure new directories do not overwrite previous ones ~ Sabina awesomeness\n",
    "def fix_valid_test_dirs(validation_dir_list, test_dir):\n",
    "    os.makedirs(validation_dir_list[0], exist_ok=True)\n",
    "    os.makedirs(validation_dir_list[1], exist_ok=True)\n",
    "    os.makedirs(validation_dir_list[2], exist_ok=True)\n",
    "    os.makedirs(validation_dir_list[3], exist_ok=True)\n",
    "    \n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "fix_valid_test_dirs([cloudy_valid, desert_valid, forest_valid, water_valid], imbalanced_test_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lightseagreen\">2. Data Cleaning<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On examination of the folders' contents, it was made clear that the cloudy images all have the prefix \"train\" since the\n",
    "# models will be trained and validated using samples of all images and for the avoidance of confusion, renaming them seems only right\n",
    "print(\"Cloudy image title check:\", sample(cloudy_img_lst, 5), \"\\n\")\n",
    "print(\"Desert image title check:\", sample(desert_img_lst, 5), \"\\n\")\n",
    "print(\"Green area title prefix check:\", sample(forest_img_lst, 5), \"\\n\")\n",
    "print(\"Water image title check:\", sample(water_img_lst, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-checking\n",
    "pattern1 = r\"train_(\\d+)\"  # pattern provided by ChatGPT\n",
    "pattern2 = r\"cloudy_(\\d+)\"\n",
    "num_match1 = 0\n",
    "num_match2 = 0\n",
    "\n",
    "# iterating\n",
    "for filename in cloudy_img_lst:\n",
    "    match1 = re.match(pattern1, random.choice(cloudy_img_lst))\n",
    "    match2 = re.match(pattern2, random.choice(cloudy_img_lst))\n",
    "    if match1:\n",
    "        num_match1 += 1\n",
    "    elif match2:\n",
    "        num_match2 += 1\n",
    "    else:\n",
    "        print(\"no match\", filename)\n",
    "\n",
    "print(\"num train\", num_match1)\n",
    "print(\"num cloudy\", num_match2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally changing names of files in direcrory\n",
    "for filename in cloudy_img_lst:\n",
    "    match = re.match(pattern1, filename)\n",
    "    if match:\n",
    "        num = match.group(1)  # keeping number part intact\n",
    "        new_filename = f\"cloudy_{num}.jpg\"  # defining new filename\n",
    "        os.rename(os.path.join(cloudy_train + \"/\" + filename), (cloudy_train + \"/\" + new_filename))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# redefining cloudy image list with new file names\n",
    "cloudy_img_lst = os.listdir(\"data/train/cloudy\")\n",
    "\n",
    "# double-checking\n",
    "print(\"Cloudy image title check:\", sample(cloudy_img_lst, 5), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also changing the name of \"SeaLake\" images to \"water\" to avoid confusion\n",
    "pattern1 = r\"SeaLake_(\\d+)\"  # pattern provided by ChatGPT\n",
    "\n",
    "for filename in water_img_lst:\n",
    "    match = re.match(pattern1, filename)\n",
    "    if match:\n",
    "        num = match.group(1)  # keeping number part intact\n",
    "        new_filename = f\"water_{num}.jpg\"  # defining new filename\n",
    "        os.rename(os.path.join(water_train + \"/\" + filename), (water_train + \"/\" + new_filename))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# redefining cloudy image list with new file names\n",
    "water_img_lst = os.listdir(\"data/train/water\")\n",
    "\n",
    "# double-checking\n",
    "print(\"Water image title check:\", sample(water_img_lst, 5), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"count_cloudy:\", len(cloudy_img_lst))\n",
    "print(\"count_desert:\", len(desert_img_lst))\n",
    "print(\"count_water:\", len(water_img_lst))\n",
    "print(\"count_cloudy:\", len(water_img_lst), \"\\n\")\n",
    "print(\"count_all:\", len(cloudy_img_lst + desert_img_lst + forest_img_lst + water_img_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe with image names and categories\n",
    "image_df = pd.DataFrame(columns = [\"img_name\", \"img_class\"])\n",
    "\n",
    "# list with all image names\n",
    "img_list = cloudy_img_lst + desert_img_lst + forest_img_lst + water_img_lst\n",
    "\n",
    "# filling out image name column\n",
    "image_df[\"img_name\"] = img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns to fill \"class\" column\n",
    "pattern_cl = r\"^cloudy\"\n",
    "pattern_des = r\"^desert\"\n",
    "pattern_gr = r\"^Forest\"\n",
    "pattern_wa = r\"^water\"\n",
    "\n",
    "# filling out class column\n",
    "for file_name in image_df.img_name:\n",
    "    # match objects or None\n",
    "    match1 = re.search(pattern_cl, str(file_name))\n",
    "    match2 = re.search(pattern_des, str(file_name))\n",
    "    match3 = re.search(pattern_gr, str(file_name))\n",
    "    match4 = re.search(pattern_wa, str(file_name))\n",
    "    if match1:\n",
    "        image_df[\"img_class\"].loc[image_df[\"img_name\"]==file_name] = str(match1.group(0))\n",
    "    elif match2:\n",
    "        image_df[\"img_class\"].loc[image_df[\"img_name\"]==file_name] = str(match2.group(0))\n",
    "    elif match3:\n",
    "        image_df[\"img_class\"].loc[image_df[\"img_name\"]==file_name] = str(match3.group(0))\n",
    "    elif match4:\n",
    "        image_df[\"img_class\"].loc[image_df[\"img_name\"]==file_name] = str(match4.group(0))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if all rows were filled\n",
    "image_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # taking a closer look into the filenames\n",
    "# for name in image_df.img_name:\n",
    "#     sys.stdout.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"dimgray\">Some of the images in the desert folder seem to be duplicated as pairs like \"desert(1007).jpg\" and \"desert(1007) (1).jpg\" exist. What is more, the filenames of only this folder are written in a different folder<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking visually\n",
    "ipimage(desert_train + \"/\" + \"desert(1007).jpg\", width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipimage(desert_train + \"/\" + \"desert(1007) (1).jpg\", width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-checking with code\n",
    "try:\n",
    "    ipimage(desert_train + \"/\" + \"desert(1007).jpg\", width=200) == ipimage(desert_train + \"/\" + \"desert(1007) (1).jpg\", width=200)\n",
    "except:\n",
    "    print(\"Not the same\")\n",
    "else:\n",
    "    print(\"The same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disposing of spaces in names and standardizing name format\n",
    "pattern_s = r\"\\s\\(\\d+\\)\" # regex writtern with the help of ChatGPT\n",
    "\n",
    "# Dirs to iterate upon\n",
    "list_paths_train = [cloudy_train, desert_train, forest_train, water_train]\n",
    "\n",
    "# Iterating on each path\n",
    "for the_dir in list_paths_train:\n",
    "    # Iterating for each filename in the dataframe \n",
    "    for filename in image_df.img_name:\n",
    "        # File directory\n",
    "        the_file = the_dir + \"/\" + filename\n",
    "        # Checking first if file directory exists\n",
    "        if os.path.exists(the_file):\n",
    "            # If exists check if pattern matches\n",
    "            match = re.search(pattern_s, filename)\n",
    "            if match:\n",
    "                # Removing image from dir\n",
    "                os.remove(the_file)\n",
    "                # Removing its record from image_df\n",
    "                image_df = image_df.drop(image_df.loc[image_df[\"img_name\"] == filename].index)\n",
    "            elif not match:\n",
    "                # If image is no duplicate, just standardize its name\n",
    "                new_filename = filename.replace(\"(\", \"_\", 1).replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "                os.rename(os.path.join(the_dir + \"/\" + filename), (the_dir + \"/\" + new_filename))\n",
    "        # If path doesn't exist, i.e. file is in different path\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying name standardization to the df\n",
    "for column in image_df.columns:\n",
    "    image_df[column] = image_df[column].apply(lambda x : str(x).replace(\"(\", \"_\", 1).replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "\n",
    "image_df = image_df.reset_index(drop = True)\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid repeating all the file operations and directory management for each new session\n",
    "image_df.to_csv(\"data/image_df.csv\", index=0)\n",
    "image_df = pd.read_csv(\"data/image_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lightseagreen\">3. Addressing Data Imbalance<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the value counts\n",
    "image_df[\"img_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"dimgray\">The desert class was already under-represented, but after the removal of duplicate images, it  became even more so.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"dimgray\">2 folders will be used. One for the baseline models using the unbalanced data and one for the models using the balanced data.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating path\n",
    "data_balance = os.path.join(os.getcwd(), \"data_balance\")\n",
    "\n",
    "# Copying data\n",
    "dest = shutil.copytree(data_path, data_balance)\n",
    "\n",
    "# Checking that tree has been copied\n",
    "print(os.listdir(data_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining new paths for the image folders\n",
    "cloudy_btrain = os.path.join(data_balance, \"train/cloudy\")\n",
    "desert_btrain = os.path.join(data_balance, \"train/desert\")\n",
    "forest_btrain = os.path.join(data_balance, \"train/green_area\")\n",
    "water_btrain = os.path.join(data_balance, \"train/water\")\n",
    "\n",
    "# defining new validation directories to be created\n",
    "cloudy_bvalid = os.path.join(data_balance, \"valid/cloudy\")\n",
    "desert_bvalid = os.path.join(data_balance, \"valid/desert\")\n",
    "forest_bvalid = os.path.join(data_balance, \"valid/green_area\")\n",
    "water_bvalid = os.path.join(data_balance, \"valid/water\")\n",
    "\n",
    "# defining new test directory to be created\n",
    "balanced_test_dir = os.path.join(data_balance, \"test\")\n",
    "\n",
    "# fixing them up so they don't get overwritten\n",
    "still_test_dir([cloudy_bvalid, desert_bvalid, forest_bvalid, water_bvalid], balanced_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the data in this case can easily be done by limiting the number of files to process\n",
    "#  for the over-represented classes to that of the under-represented one\n",
    "# A simple way to do that is to sample the data using the index at our disposal (image_df)\n",
    "sample_cloudy = image_df[image_df[\"img_class\"]==\"cloudy\"].sample(1092)\n",
    "sample_forest = image_df[image_df[\"img_class\"]==\"forest\"].sample(1092)\n",
    "sample_water = image_df[image_df[\"img_class\"]==\"water\"].sample(1092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the collection together\n",
    "balanced_df = pd.concat([sample_cloudy, sample_forest, sample_water, image_df[image_df[\"img_class\"]==\"desert\"]], axis = 0).sample(frac = 1).reset_index(drop = True)\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"img_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(\"data_balance/balanced_df.csv\", index = 0)\n",
    "balanced_df = pd.read_csv(\"data_balance/balanced_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's tidy up the folders by deleting images not existing in the dataframe\n",
    "dir_bcloudy = os.listdir(cloudy_btrain)\n",
    "dir_bdesert = os.listdir(desert_btrain)\n",
    "dir_bgreen = os.listdir(forest_btrain)\n",
    "dir_bwater = os.listdir(water_btrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirs to iterate upon\n",
    "paths_list = [cloudy_btrain, desert_btrain, forest_btrain, water_btrain]\n",
    "folders = [dir_bcloudy, dir_bdesert, dir_bgreen, dir_bwater]\n",
    "\n",
    "# Iterating on each folder dir\n",
    "for i in range(0,len(folders)):\n",
    "    folder = folders[i]\n",
    "    path = paths_list[i]\n",
    "    # Iterating on file in the folder\n",
    "    for folder_filename in folder:\n",
    "        # File directory\n",
    "        the_file = path + \"/\" + folder_filename\n",
    "        # Checking first if file directory exists\n",
    "        if os.path.exists(the_file):\n",
    "            # If exists in list\n",
    "            if folder_filename in balanced_df.img_name.unique().tolist():\n",
    "                # Keep image\n",
    "                continue\n",
    "            else:\n",
    "                os.remove(the_file)\n",
    "        # If path doesn't exist, i.e. file is in different path\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lightseagreen\">4. Train-test split<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for moving images\n",
    "def move_img(img_list, source, destination):\n",
    "    for name in img_list:\n",
    "        #if name != '.ipynb_checkpoints':\n",
    "        shutil.move(source + \"/\" + name, destination +  \"/\" + name)\n",
    "        #else:\n",
    "            #continue\n",
    "\n",
    "            \n",
    "            \n",
    "# Function to sample images from the different classes\n",
    "def names_per_class(df, class_column):\n",
    "    # sorted list of classes\n",
    "    class_list = df[class_column].unique()\n",
    "    class_list.sort()\n",
    "    \n",
    "    # list to store lists\n",
    "    class_nested_lists = []\n",
    "    \n",
    "    # storing lists in class_nested_lists\n",
    "    for each_class in class_list:\n",
    "        class_iimages = list(df[df[class_column]==each_class][name_column])\n",
    "        nested_class_lists.append(class_images)\n",
    "    \n",
    "    return nested_class_lists\n",
    "\n",
    "\n",
    "\n",
    "# Function to sample names following names_per_class\n",
    "def name_sampler(class_nested_lists, percentage):\n",
    "    \n",
    "    # list to store samples\n",
    "    nested_samples = []\n",
    "    \n",
    "    for each_list in nested_class_lists:\n",
    "        list_class_valid_sample = sample(each_list, int(len(each_list) * percentage))\n",
    "        nested_samples.append(list_class_valid_sample)\n",
    "    \n",
    "    return nested_samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train-test split\n",
    "#### CAN USE IMPROVEMENTS --> ITERATE ON CLASS_LISTS AND SAMPLES, APPEND THEM TO LIST, SAME FOR TUPLES, ADD PART AND ARGUMENTS THAT CREATE TUPLES IN FUNCTION ####\n",
    "############ EVEN MORE IMPROVEMENTS? --> WRITE FUNCTION THAT PICKS SAMPLE FROM IMAGE FOLDERS, BUT DOESN'T NED TO CREATE NEW DIRECTORIES TO WORK, BUT INSTEAD USES NAMES OF FILES TO ACCESS THEM FOR THE MODEL EACH TIME ###############\n",
    "def homemade_split(df, class_column, name_column, valid_source_dest_tuple_list, test_dest, sample_percent):\n",
    "    \n",
    "    # sampling image names into lists\n",
    "    nested_lists_valid = names_per_class(df, class_column)\n",
    "\n",
    "######### FIX PERCENTAGE ############\n",
    "    # Storing random sample of ??? of names of items to be moved  in validation folder in new lists\n",
    "    samples_valid = name_sampler(nested_lists_valid, sample_percent)\n",
    "    \n",
    "    # Moving images corresponding to valid samples\n",
    "    #samples = [list_class_1_valid_sample, list_class_2_valid_sample, list_class_3_valid_sample, list_class_4_valid_sample]\n",
    "\n",
    "    # creating tuples with (sample, source, destination) for validation data\n",
    "    tuple_1 = samples_valid[0], source_dest_tuple_list[0][0], source_dest_tuple_list[0][1]\n",
    "    tuple_2 = samples_valid[1], source_dest_tuple_list[1][0], source_dest_tuple_list[1][1]\n",
    "    tuple_3 = samples_valid[2], source_dest_tuple_list[2][0], source_dest_tuple_list[2][1]\n",
    "    tuple_4 = samples_valid[3], source_dest_tuple_list[3][0], source_dest_tuple_list[3][1]\n",
    "    \n",
    "    # putting tuples in a list\n",
    "    tuples = [tuple_1, tuple_2, tuple_3, tuple_4]\n",
    "    \n",
    "    # moving to validation dirs\n",
    "    for tuple_trio in tuples:\n",
    "        move_img(tuple_trio[0], tuple_trio[1], tuple_trio[2])\n",
    "    \n",
    "    # merging sample lists\n",
    "    valid_list = [*samples_valid[0], *samples_valid[1], *samples_valid[2], *samples_valid[3]]\n",
    "\n",
    "    # creating valid_df\n",
    "    valid_df = df[df[name_column].isin(valid_list)].reset_index(drop=True)\n",
    "\n",
    "    # creating train_df\n",
    "    train_df_1 = df[~df[name_column].isin(valid_list)].reset_index(drop=True)\n",
    "\n",
    "######### FIX PERCENTAGE ############\n",
    "    # repeating above steps for test folder\n",
    "    nested_lists_test = names_per_class(train_df_1, class_column)\n",
    "    samples_test = name_sampler(nested_lists_test, sample_percent)\n",
    "    test_list = [*samples_test[0], *samples_test[1], *samples_test[2], *samples_test[3]]\n",
    "    \n",
    "    # create source path list\n",
    "    source_path_list = [source_dest_tuple_list[0][0], source_dest_tuple_list[1][0], source_dest_tuple_list[2][0], source_dest_tuple_list[1][0]]\n",
    "    \n",
    "    # moving to test dir\n",
    "    for source_path in source_path_list:\n",
    "        move_img(test_list, source_path, test_dest)\n",
    "    \n",
    "    # creating test_df and redefining train_df\n",
    "    test_df = df[df[name_column].isin(test_list)].reset_index(drop=True)\n",
    "    train_df_2 = train_df[~train_df[name_column].isin(test_list)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df_2, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the function above is dependent on a sorted list of class names to function correctly, let's see how the sorted list looks like and write our tuples list accordingly\n",
    "class_list = image_df[\"img_class\"].unique()\n",
    "class_list.sort()\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (source, destination) tuples for imbalanced data\n",
    "imb_cl_tuple = cloudy_train, cloudy_valid\n",
    "imb_des_tuple = desert_train, desert_valid\n",
    "imb_gr_tuple = forest_train, forest_valid\n",
    "imb_wat_tuple = water_train, water_valid\n",
    "\n",
    "# Tuple list for imbalanced validation data\n",
    "imbalanced_tuple_list = [imb_cl_tuple, imb_des_tuple, imb_gr_tuple, imb_wat_tuple]\n",
    "\n",
    "# Source/destination tuple for test data\n",
    "imbalanced_test_tuple = \n",
    "\n",
    "# Splitting imbalanced data\n",
    "imbalanced_train, imbalanced_valid = homemade_split(image_df, \"img_class\", \"img_name\", imbalanced_tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indentation trick found at https://stackoverflow.com/questions/18756510/printing-with-indentation-in-python\n",
    "print(\"Length of train set:\", len(imbalanced_train), \"of which\", \"\\n\",\n",
    "      f\"{'':<20}{len(imbalanced_train[imbalanced_train['img_class']=='cloudy'])}\", \"cloudy\", \"\\n\",\n",
    "         f\"{'':<21}{len(imbalanced_train[imbalanced_train['img_class']=='desert'])}\", \"desert\", \"\\n\", \n",
    "         f\"{'':<20}{len(imbalanced_train[imbalanced_train['img_class']=='forest'])}\", \"forest\", \"\\n\",\n",
    "         f\"{'':<20}{len(imbalanced_train[imbalanced_train['img_class']=='water'])}\", \"water\")\n",
    "\n",
    "print(\"Length of validation set:\", len(imbalanced_valid[\"img_class\"]), \"of which\", \"\\n\",\n",
    "      f\"{'':<20}{len(imbalanced_valid[imbalanced_valid['img_class']=='cloudy'])}\", \"cloudy\", \"\\n\",\n",
    "         f\"{'':<20}{len(imbalanced_valid[imbalanced_valid['img_class']=='desert'])}\", \"desert\", \"\\n\", \n",
    "         f\"{'':<20}{len(imbalanced_valid[imbalanced_valid['img_class']=='forest'])}\", \"forest\", \"\\n\",\n",
    "         f\"{'':<20}{len(imbalanced_valid[imbalanced_valid['img_class']=='water'])}\", \"water\", \"\\n\")\n",
    "\n",
    "print(\"Length of test set:\", f\"{'':<20}{len(imbalanced_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file operations are costly timewise and must be precise. saving dfs to csv to avoid having to use listdir with each new session\n",
    "imbalanced_train.to_csv(\"data/imbalanced_train.csv\", index = 0)\n",
    "imbalanced_valid.to_csv(\"data/imbalanced_valid.csv\", index = 0)\n",
    "imbalanced_test.to_csv(\"data/imbalanced_valid.csv\", index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_train = pd.read_csv(\"data/imbalanced_train.csv\")\n",
    "imbalanced_valid = pd.read_csv(\"data/imbalanced_valid.csv\")\n",
    "imbalanced_test = pd.read_csv(\"data/imbalanced_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better safe than sorry\n",
    "class_list = balanced_df[\"img_class\"].unique()\n",
    "class_list.sort()\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (source, destination) tuples for balanced data\n",
    "bal_cl_tuple = cloudy_btrain, cloudy_bvalid\n",
    "bal_des_tuple = desert_btrain, desert_bvalid\n",
    "bal_gr_tuple = forest_btrain, forest_bvalid\n",
    "bal_wat_tuple = water_btrain, water_bvalid\n",
    "\n",
    "# Tuple list for balanced data\n",
    "balanced_tuple_list = [bal_cl_tuple, bal_des_tuple, bal_gr_tuple, bal_wat_tuple]\n",
    "\n",
    "# Splitting balanced data\n",
    "balanced_train, balanced_valid = homemade_split(balanced_df, \"img_class\", \"img_name\", balanced_tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # listing images of the 4 classes\n",
    "# cloudy_blist = list(balanced_df[balanced_df[\"img_class\"]==\"cloudy\"][\"img_name\"])\n",
    "# desert_blist = list(balanced_df[balanced_df[\"img_class\"]==\"desert\"][\"img_name\"])\n",
    "# greenery_blist = list(balanced_df[balanced_df[\"img_class\"]==\"forest\"][\"img_name\"])\n",
    "# water_blist = list(balanced_df[balanced_df[\"img_class\"]==\"water\"][\"img_name\"])\n",
    "\n",
    "# # Storing random sample of 25% of names of items to be moved in new lists\n",
    "# cloudy_btest_sample = sample(cloudy_blist, int((len(cloudy_blist) * .25)))\n",
    "# desert_btest_tsample = sample(desert_blist, int((len(desert_blist) * .25)))\n",
    "# greenery_btest_tsample = sample(greenery_blist, int((len(greenery_blist) * .25)))\n",
    "# water_btest_tsample = sample(water_blist, int((len(water_blist) * .25)))\n",
    "\n",
    "# # Moving the images\n",
    "# samples = [cloudy_btest_sample, desert_btest_tsample, greenery_btest_tsample, water_btest_tsample]\n",
    "\n",
    "# move_img(cloudy_btest_sample, cloudy_btrain, cloudy_btest)\n",
    "# move_img(desert_btest_tsample, desert_btrain, desert_btest)\n",
    "# move_img(greenery_btest_tsample, greenery_btrain, greenery_btest)\n",
    "# move_img(water_btest_tsample, water_btrain, water_btest)\n",
    "\n",
    "# # Separating train-test dataframes\n",
    "# test_list = [*cloudy_btest_sample, *desert_btest_tsample, *greenery_btest_tsample, *water_btest_tsample]\n",
    "\n",
    "# # Creating test_df\n",
    "# test_df = balanced_df[balanced_df[\"img_name\"].isin(test_list)].reset_index(drop=True)\n",
    "\n",
    "# # Creating train_df\n",
    "# train_df = balanced_df[~balanced_df[\"img_name\"].isin(test_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to access already processed directories of balanced\n",
    "# dir_bcloudy_train = os.listdir(cloudy_btrain)\n",
    "# dir_bdesert_train = os.listdir(desert_btrain)\n",
    "# dir_bgreen_train = os.listdir(greenery_btrain)\n",
    "# dir_bwater_train = os.listdir(water_btrain)\n",
    "\n",
    "# balanced_train_list = [*dir_bcloudy_train, *dir_bdesert_train, *dir_bgreen_train, *dir_bwater_train]\n",
    "\n",
    "# dir_bcloudy_test = os.listdir(cloudy_btest)\n",
    "# dir_bdesert_test = os.listdir(desert_btest)\n",
    "# dir_bgreen_test = os.listdir(greenery_btest)\n",
    "# dir_bwater_test = os.listdir(water_btest)\n",
    "\n",
    "# balanced_test_list = [*dir_bcloudy_test, *dir_bdesert_test, *dir_bgreen_test, *dir_bwater_test]\n",
    "\n",
    "# # # Creating train_df\n",
    "# balanced_train = balanced_df[~balanced_df[\"img_name\"].isin(balanced_train_list)].reset_index(drop=True)\n",
    "\n",
    "# # # Creating test_df\n",
    "# balanced_test = balanced_df[balanced_df[\"img_name\"].isin(balanced_test_list)].reset_index(drop=True)\n",
    "\n",
    "# file operations are costly timewise and must be precise. saving dfs to csv to avoid having to use listdir with each new session\n",
    "# balanced_train.to_csv(\"data_balance/balanced_train.csv\", index = 0)\n",
    "# balanced_test.to_csv(\"data_balance/balanced_test.csv\", index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indentation trick found at https://stackoverflow.com/questions/18756510/printing-with-indentation-in-python\n",
    "print(\"Length of validation set:\", len(balanced_valid), \"of which\", \"\\n\",\n",
    "      f\"{'':<20}{len(balanced_valid[balanced_valid['img_class']=='cloudy'])}\", \"cloudy\", \"\\n\",\n",
    "         f\"{'':<20}{len(balanced_valid[balanced_valid['img_class']=='desert'])}\", \"desert\", \"\\n\", \n",
    "         f\"{'':<20}{len(balanced_valid[balanced_valid['img_class']=='forest'])}\", \"forest\", \"\\n\",\n",
    "         f\"{'':<20}{len(balanced_valid[balanced_valid['img_class']=='water'])}\", \"water\", \"\\n\")\n",
    "print(\"Length of train set:\", len(balanced_train), \"of which\", \"\\n\",\n",
    "      f\"{'':<21}{len(balanced_train[balanced_train['img_class']=='cloudy'])}\", \"cloudy\", \"\\n\",\n",
    "         f\"{'':<21}{len(balanced_train[balanced_train['img_class']=='desert'])}\", \"desert\", \"\\n\", \n",
    "         f\"{'':<21}{len(balanced_train[balanced_train['img_class']=='forest'])}\", \"forest\", \"\\n\",\n",
    "         f\"{'':<21}{len(balanced_train[balanced_train['img_class']=='water'])}\", \"water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train = pd.read_csv(\"data_balance/balanced_train.csv\")\n",
    "balanced_valid = pd.read_csv(\"data_balance/balanced_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color:lightseagreen\">5. Modeling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM ISI'S SCRIPT\n",
    "# ðŸŽ¯ Specific functions\n",
    "def make_model(input_shape): \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (1, 1), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (2, 2), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color:lightseagreen\">5.1 Image Pre-processing - CNN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM ISI'S SCRIPT\n",
    "# Parameters that we can fine-tune later on\n",
    "img_height = 256   \n",
    "img_width = 256   \n",
    "image_size = (img_height, img_width)\n",
    "batch_size = 128 # using the same as in the tutorial, training utilized in each iteration\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color:lightseagreen\">5.2.1 Baseline Models: Keras - CNN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM ISI'S\n",
    "# Loading training data\n",
    "train_ds = datagen.flow_from_directory(\n",
    "    train_dir, # training directory\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Loading validation data\n",
    "val_ds = datagen.flow_from_directory(\n",
    "    val_dir, # validation directory\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISI'S\n",
    "# Keras model, build on top of TensorFlow\n",
    "model = make_model(input_shape=image_size + (3,)) # Image size + 3 channels of colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING MODEL - ISI'S\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adamax(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING MODEL - ISI'S\n",
    "epochs = 30\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"data/epochs/save_at_{epoch}.keras\"),\n",
    "]\n",
    "\n",
    "# Train your model without callbacks first\n",
    "hist = model.fit_generator(\n",
    "    train_ds, \n",
    "    epochs=epochs, \n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
